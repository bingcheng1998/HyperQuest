{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Implementing a Neural Network\n",
    "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.classifiers.neural_net_b import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "        del X_train, y_train\n",
    "        del X_test, y_test\n",
    "        print('Clear previously loaded data.')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "- Save data to pickle file\n",
    "- Draw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "def showTraining(stats):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(stats['train_acc_history'], label='train')\n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Classification accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def show_net_weights(W1):\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "\n",
    "def savedb(obj,filename):\n",
    "    with open(filename,'wb') as file:\n",
    "        pickle.dump(obj,file)\n",
    "    \n",
    "def loaddb(filename):\n",
    "    with open(filename,'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "        return obj\n",
    "    \n",
    "def pickle_exist(hs, bs, lr, reg, num_epoch):\n",
    "    filename = f'pickle/{hs}-{bs}-{lr}-{reg}-{num_epoch}.pickle'\n",
    "    if os.path.isfile(filename):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def save_pickle(hs, bs, lr, reg, num_epoch, val_acc, W1, stats, dtype = np.half):\n",
    "    W1 = dtype(W1)\n",
    "    for key in stats.keys():\n",
    "        stats[key] = dtype(stats[key])\n",
    "    obj = (hs, bs, lr, reg, num_epoch, val_acc, W1, stats)\n",
    "    filename = f'pickle/{hs}-{bs}-{lr}-{reg}-{num_epoch}.pickle'\n",
    "    savedb(obj,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your hyperparameters\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Approximate results**. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.\n",
    "\n",
    "**Experiment**: You goal in this exercise is to get as good of a result on CIFAR-10 as you can (52% could serve as a reference), with a fully-connected Neural Network. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................................\n",
      "................................................................."
     ]
    }
   ],
   "source": [
    "best_net = None \n",
    "best_state = None\n",
    "input_size = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "best_acc = 0\n",
    "\n",
    "hidden_size = [3, 5, 10, 50, 150]\n",
    "# batch_size = [10, 50, 100, 200, 300, 400]\n",
    "batch_size = [10, 50, 100, 200, 400]\n",
    "# learning_rate = [4.5e-3, 4e-3, 3.5e-3, 3e-3, 2e-3, 1e-3, 5e-4, 1e-4]\n",
    "learning_rate = [4e-3, 3e-3, 1e-3, 5e-4, 1e-4]\n",
    "# regularization = [0, 0.1, 0.5, 1, 3, 5, 10]\n",
    "regularization = [0, 0.1, 0.5, 1, 10]\n",
    "num_epoch = 8\n",
    "\n",
    "# hidden_size = [10]\n",
    "# batch_size = [200]\n",
    "# learning_rate = [4.5e-3, 4e-3, 3.5e-3, 3e-3, 2e-3]\n",
    "# regularization = [0.5]\n",
    "# num_epoch = 8\n",
    "\n",
    "best_hs = hidden_size[0]\n",
    "best_bs = batch_size[0]\n",
    "best_lr = learning_rate[0]\n",
    "best_reg = regularization[0]\n",
    "\n",
    "def testParam(hs, bs, lr, reg):\n",
    "    net = TwoLayerNet(input_size, hs, num_classes, dtype = np.single)\n",
    "    num_train = X_train.shape[0]\n",
    "    iterations_per_epoch = max(num_train//bs, 1)\n",
    "    num_iters = int(np.ceil(iterations_per_epoch*num_epoch)+1)\n",
    "    \n",
    "    stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                num_iters=num_iters, batch_size=bs,\n",
    "                learning_rate=lr, learning_rate_decay=0.95,\n",
    "                reg=reg, verbose=False)\n",
    "    val_acc = (net.predict(X_val) == y_val).mean()\n",
    "    print(f'hs: {hs}, bs: {bs}, lr: {lr}, reg:{reg}, val_acc: {val_acc}')\n",
    "    return val_acc, net, stats\n",
    "\n",
    "i = 0\n",
    "for lr in reversed(learning_rate):\n",
    "    for bs in batch_size:\n",
    "        for hs in hidden_size:\n",
    "            for reg in regularization:\n",
    "                if pickle_exist(hs, bs, lr, reg, num_epoch):\n",
    "                    print('.',end='')\n",
    "                    if i%80 == 79:\n",
    "                        print()\n",
    "                    i+=1\n",
    "                    continue\n",
    "                val_acc, net, stats = testParam(hs, bs, lr, reg)\n",
    "                W1 = net.params['W1']\n",
    "                save_pickle(hs, bs, lr, reg, num_epoch, val_acc, W1, stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
